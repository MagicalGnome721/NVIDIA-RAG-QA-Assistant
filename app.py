import os
import chainlit as cl
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.memory import ChatMemoryBuffer

# =======================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG AI
# =======================================================

# L·∫•y ƒë·ªãa ch·ªâ Ollama (M·∫∑c ƒë·ªãnh localhost)
ollama_url = os.getenv("OLLAMA_HOST", "http://localhost:11434")
print(f"--- ƒêang k·∫øt n·ªëi t·ªõi Ollama t·∫°i: {ollama_url} ---")

Settings.llm = Ollama(
    model="llama3",
    base_url=ollama_url,
    request_timeout=300.0,
    system_prompt="""
    B·∫°n l√† Tr·ª£ l√Ω Tuy·ªÉn d·ª•ng ·∫£o c·ªßa NVIDIA.
    NHI·ªÜM V·ª§:
    1. Tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa ·ª©ng vi√™n d·ª±a tr√™n JD (Job Description) ƒë∆∞·ª£c cung c·∫•p.
    2. Nh·∫•n m·∫°nh v√†o y√™u c·∫ßu v·ªÅ: GPU, CUDA, Testing v√† Automation.
    3. Lu√¥n t·ªè ra chuy√™n nghi·ªáp, ng·∫Øn g·ªçn v√† khuy·∫øn kh√≠ch ·ª©ng vi√™n n·ªôp CV.
    N·∫øu kh√¥ng c√≥ th√¥ng tin trong JD, h√£y n√≥i: "Th√¥ng tin n√†y kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong b·∫£n m√¥ t·∫£ c√¥ng vi·ªác."
    """
)

# C·∫•u h√¨nh Model nh√∫ng (Embedding)
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

# =======================================================
# 2. KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG
# =======================================================

@cl.on_chat_start
async def start():
    # S·ª≠a 1: T√™n Bot chuy√™n nghi·ªáp h∆°n
    msg = cl.Message(content="üöÄ **NVIDIA HR Assistant ƒëang kh·ªüi ƒë·ªông...**")
    await msg.send()

    # Ki·ªÉm tra folder data
    if not os.path.exists("./data") or not os.listdir("./data"):
        await cl.Message(content="‚ö†Ô∏è Folder `data` ƒëang tr·ªëng ho·∫∑c kh√¥ng t·ªìn t·∫°i. H√£y copy file JD v√†o ƒë√≥.").send()
        return

    try:
        # ƒê·ªçc d·ªØ li·ªáu t·ª´ folder
        documents = SimpleDirectoryReader("./data").load_data()
        
        # T·∫°o Index
        index = VectorStoreIndex.from_documents(documents)
        
        # T·∫°o b·ªô nh·ªõ
        memory = ChatMemoryBuffer.from_defaults(token_limit=3000)

        # T·∫°o Chat Engine
        chat_engine = index.as_chat_engine(
            chat_mode="context",
            memory=memory,
            system_prompt=Settings.llm.system_prompt,
            similarity_top_k=3
        )
        
        # L∆∞u session
        cl.user_session.set("chat_engine", chat_engine)

        # S·ª≠a 2: C√¢u l·ªánh chu·∫©n, kh√¥ng b·ªã l·ªói c√∫ ph√°p
        msg.content = f"‚úÖ **S·∫µn s√†ng!**\nƒê√£ k·∫øt n·ªëi Ollama t·∫°i `{ollama_url}`.\nƒê√£ h·ªçc xong JD v·ªã tr√≠ SWQA. M·ªùi b·∫°n ƒë·∫∑t c√¢u h·ªèi v·ªÅ c√¥ng vi·ªác!"
        await msg.update()
        
    except Exception as e:
        error_msg = f"‚ùå L·ªói kh·ªüi ƒë·ªông: {str(e)}"
        if "Connection refused" in str(e):
            error_msg += "\n\nüí° G·ª£i √Ω: Ki·ªÉm tra xem Ollama ƒë√£ b·∫≠t ch∆∞a?"
        await cl.Message(content=error_msg).send()

# =======================================================
# 3. X·ª¨ L√ù TIN NH·∫ÆN
# =======================================================

@cl.on_message
async def main(message: cl.Message):
    chat_engine = cl.user_session.get("chat_engine")
    
    if not chat_engine:
        await cl.Message(content="‚ö†Ô∏è H·ªá th·ªëng ch∆∞a s·∫µn s√†ng. H√£y F5 l·∫°i trang.").send()
        return

    msg = cl.Message(content="")
    
    # G·ªçi AI tr·∫£ l·ªùi
    response = chat_engine.stream_chat(message.content)
    
    # Hi·ªán ch·ªØ d·∫ßn d·∫ßn
    for token in response.response_gen:
        await msg.stream_token(token)

    await msg.send()